%
% File acl2014.tex
%
% Contact: giovanni.colavizza@epfl.ch
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{multirow}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Sentimental impact of world events on the Twitter social network}

\author{Nuno M. Mota Gon\c{c}alves \\
	{\tt \small nuno.motagoncalves@epfl.ch} \\\And
	Matteo Y. Feo \\
	{\tt \small matteo.feo@epfl.ch} \\\And
	Luc\'{i}a Montero Sanchis\\
	{\tt \small lucia.monterosanchis@epfl.ch} \\}

\date{}

\begin{document}
	\maketitle
	\begin{abstract}
		The aim of this project is to quantify the \textit{sentimental impact} a certain
		world event had on the Twitter community. Taking into account the emotional
		value associated with a specific event - found through \textit{Language
			Recognition (LR)}, \textit{Name Entity Recognition (NER)} and \textit{Sentiment
			Analysis (SA)} - we are now able to define the \textit{impactfullness} of
		conflicts from the Uppsala Conflict Data Program (UCDP) on a scale from 0.0 to
		1.0. At this stage, country-wide events are the most viably described by our
		system, although with further developments (and time) it could be fine-tuned
		with more sources of data. The preliminary results seem to be pretty consistent
		and, further down the line, we can easily foresee the development of a
		\textit{sentimental impact} prediction model for future events.
	\end{abstract}
	
	\section{Introduction}
	
	For our project we initially decided to use both UCDP and Twitter-leon
	datasets. Because of time-constraints, however, we ended replacing the latter
	with our own Twitter dataset created using a Markov Chains based model. This
	model, trained on stories, geo-political and religious texts, basically
	generates Tweets on several different subjects.
	
	From the combination of the UCDP and Twitter datasets we were now using, we
	wanted to understand how we could match certain Tweets to a given event and then
	evaluate the emotional value of the event itself - based on the retrieved
	information. By using \textit{LR}, \textit{NER} and \textit{SA}, although
	limited to the English language, we manged to draw the desired correspondence.
	It's important to note we made the assumption that the content of English Tweets
	alone is a good representation of the world's overall opinion (which may
	introduce a Western Country preference to the analysis).
	
	As to how we will make that information clear, we decided to showcase scenarios
	in which we pipeline our methodology and discuss the pros and cons of our
	approach to the problem - through statistics and plotting. In the end, every
	scenario will have its \textit{sentimental impact} evaluated and compared to
	other scenarios.
	
	To describe our work in more detail, we split our report into several sections: (2) Data Collection - how we got a hold of our data; (3) Data set description - where we both state which datasets we used for our project and describe our initial treatment of their data; (4) Methodology - showing how we made use of certain libraries and our datasets to achieve our goal; (5) Results and Findings - contains some considerations about certain parts of the project; and (6) Conclusions - our final remarks and future improvements on this idea.
	
	
	\section{Data Collection}
	Since we ended up not using the Twitter-leon dataset, all the data we used came from locally downloaded datasets. All of them are public and where acquired in either CSV or JSON format.
	
	\section{Dataset Description}
	\subsection{UCDP Dataset}
	\label{sub:ucdp_dataset}
	The UCDP dataset contains data for $133 012$ worldwide conflicts from 1989
	until 2016. Other information available for  conflicts relevant for our analysis
	is:
	\begin{itemize}
		\item Type of violence - Categorical variable that classifies conflicts into
		\emph{State-Based},  \emph{Non-State} and \emph{One-Sided} violence.
		\item Location - Coordinates, region and country are available. We are only
		interested in the country.
		\item Start and end dates - They can be used to calculate the duration of
		the conflict.
		\item Number of casualties - Both the total number and the casualties on
		each side involved are available. We consider only the total number.
	\end{itemize}
	
	\subsubsection*{UCDP Dataset Filtering}
	\label{ssub:dataset_filtering}
	We first assumed the more recent year's Twitter population should be more
	representative of nowadays reactions to the same events we are studying.
	Moreover, since the number of worldwide active users on Twitter has increased
	throughout the years ~\cite{arrojo2015social}, we ultimately decided to focus on
	conflicts that took place during 2016 - last year contained in the datasets. On
	top of that, it is also more likely that the public reacts to certain types of
	violence over others - and therefore we opted to analyze \textit{one-sided
		violence} events. This type of violence represents 13.7\% of the conflicts in
	2016, as well as 10.7\% of casualties in that year. Table \ref{tab:ucdp16stats}
	contains the statistics of these conflicts.
	
	\begin{table}[h]
		\begin{center}
			\begin{tabular}{|l|r|r|r|}
				\hline
				& \bf Casualties & \bf Duration &
				\begin{tabular}[c]{@{}c@{}}\bf N. per\\\bf country\end{tabular} \\
				\hline
				mean & 6.5 & 3 days & 22.9\\
				std & 19.9 & 21 days & 33.8\\
				min & 0 & 0 days & 1\\
				25\% & 1 & 0 days & 2\\
				50\% & 2 & 0 days & 6\\
				75\% & 5 & 0 days & 25.5\\
				max & 324 & 334 days & 106\\
				\hline
				count & \multicolumn{3}{|c|}{893}\\
				\hline
			\end{tabular}
		\end{center}
		\caption{\label{tab:ucdp16stats} Statistics for one-sided conflicts in
			2016}
	\end{table}
	
	As shown in the table, the number of casualties per conflict varies from 0
	to 324. We assume that the public reaction will be higher for conflicts with
	casualties and, as a result, only conflicts with casualties are considered.
	
	Table \ref{tab:ucdp16stats} also includes the number of conflicts per
	country, since we are assuming that the general public is likely to lose
	'interest' (meaning they probably won't show drastic changes on their behavior)
	on the violent events that take place in a country where conflicts are very
	frequent. Therefore we also only consider the countries where there were, at
	most, three one-sided violent conflicts with casualties during 2016.
	
	The statistics for the final filtered conflicts are shown in Table
	\ref{tab:ucdp16fstats}. The result is 30 conflicts from 18 different countries.
	
	\begin{table}[h]
		\begin{center}
			\begin{tabular}{|l|r|r|r|}
				\hline
				& \bf Casualties & \bf Duration &
				\begin{tabular}[c]{@{}c@{}}\bf N. per\\\bf country\end{tabular} \\
				\hline
				mean & 8 & 0 days & 1.67 \\
				std & 16.9 & 1 days & 0.69 \\
				min & 1 & 0 days & 1 \\
				25\% & 1 & 0 days & 1 \\
				50\% & 2 & 0 days & 2 \\
				75\% & 5 & 0 days & 2 \\
				max & 86 & 5 days & 3 \\
				\hline
				count & \multicolumn{3}{|c|}{30}\\
				\hline
			\end{tabular}
		\end{center}
		\caption{\label{tab:ucdp16fstats} Statistics for filtered conflicts}
	\end{table}
	
	\subsection{Other Datasets}
	\label{sub:other_datasets}
	More than identifying entities in a text using our NER model, we want to identify which countries they might be referring to specifically. To
	achieve this, we basically used several public datasets datasets (such as those present the data directory) to create bilateral word associations between: countries, cities, nationalities, religions, religious affiliations and currencies. 
	\section{Methods}
	\subsection{Language Recognition}
	\label{sub:language_recognition}
	For this stage we basically use an underlying library, langdetect, to filter out foreign tweets.
	
	\subsection{Named Entity Recognition}
	\label{sub:named_entity_recognition}
	As stated before, our main goal is to find the country a certain Tweet is talking about. To this end we use the Spacy library and its NER models trained on OntoNotes 5 (giving us plenty of labels to work with). Even though the association from a country's name or nationality might be straightforward, religions and currencies tend to reference many different countries. As such, our NER module calculates the overall probability of each country in three steps:
	\begin{enumerate}
		\item For each identified identity, use the word association to find all the countries the word might be referencing and assign each a probability of $\frac{1}{N}$, where $N$ represents the number of total countries associated.
		\item From all the country-probability sets created in the previous step, we now generate a single set based on their union. For countries that exist for several entities, we add their probabilities.
		\item Finally, we do a normalization of the probability set $p$:
		\begin{equation}
		p_i = \frac{p_{i}}{\sum_{j=1}^{|p|} p_j}
		\end{equation}
	\end{enumerate}
	
	From these probabilities, we assume that the Tweet is talking about a certain country IFF:
	\begin{equation}
	p_{i} = \max_{1 \leq j \leq |p|} p_{j} \wedge p_{i} \ge threshold
	\end{equation}
	where $threshold \in [0, 1[$.
	
	\subsection{Sentiment Analysis}
	\label{sub:sentiment_analysis}
	For sentiment analysis we used \textit{NLTK's Vader sentiment analyzer}. We then associate the composite value from its computation with the corresponding Tweet. This value will always be within $[-1, 1]$.
	
	\subsection{Daily Sentimental Strength}
	\label{sub:daily_sentiment_strength}
	Before we analyze the impact of an event, we wanted to measure the \textit{sentiment strength} on a daily basis, using all of the Tweets for that specific date. For this step, we want to penalize outliers. We mainly want to focus on the 'clustering' of the \textit{sentiment strength} over that day so random, overly-emotional tweets should not affect our final measure.
	To do that, we define an \textit{outlier} following  For a given set of values $d$:
	\begin{equation}
	f_{i} = |d_{i} - \tilde{d}|
	\end{equation} 
	\begin{equation}
	g_{i} = 0.6745 * \frac{f_{i}}{\tilde{f}}
	\end{equation}
	\begin{equation}
	h_{i} = \left\{
	\begin{array}{lr}
	1 & : g_{i} > threshold\\
	0 & : g_{i} \leq threshold
	\end{array}
	\right.
	\end{equation}
	\begin{equation}
	r = \frac{\sum_{i=1}^{|d|} d_{i} * h_{i}}{\sum_{j=1}^{|h|} h_{j}}
	\end{equation} 
	
	where $r \in [-1, 1]$ and represents the final result for our \textit{daily sentiment strength}.
	
	\subsection{Sentimental Impact Measurement}
	\label{sub:sentimental_impact_measurement}
	On this stage, however, we do not want to penalize outliers. Since we are going to calculate \textit{sentimental impact} over the \textit{daily sentiment strengths}, we assume that big differences between successive values means they were triggered by an event. Instead, we actually want higher values to strongly define the final result we get (which means there was a big impact). 
	For a set of daily averages, evenly spread around a certain date of an event, we split them into $b$ and $a$ sets - each representing the values before and after the date of the event respectively. The exact day of the event is always considered to be the first day in the $a$ set.
	\begin{equation}
	f = |\max_{1 \leq i \leq |b|} b_{i} - \min_{1 \leq i \leq |b|} b_{i}|
	\end{equation}
	\begin{equation}
	g = |\max_{1 \leq i \leq |a|} a_{i} - \min_{1 \leq i \leq |a|} a_{i}|
	\end{equation}
	\begin{equation}
	r = |g - f|
	\end{equation}
	
	where $r \in [0, 1]$ and represents our final measurement for \textit{sentimental impact}. We are aware that this measurement completely ignores sentimental 'aftershocks' (emotional fluctuations that might be visible after the occurrence of a given event) but this is a desired characteristic since we are comparing single day conflicts with multiple-day conflicts. The same behavior shifted within the time-frame (maybe a delayed response) will represent the same final value but conflicts that impact people several times (multiple-day) will not be considered more \textit{impactfull} just because of their duration.
	
	\section{Results and Findings}
	Although the NER module is pretty accurate, we identified a lot of cases in which word capitalization had direct impact on whether or not it would be able to find a specific word. Although expectable, due to the training provided by OntoNotes 5, applying capitalization operations to the whole text (e.g uppercase, lowercase and titlecase) before passing it onto the module did not improve its accuracy in any type of entity. Capitalizing only nouns would improve our results but, since the model can't identify those nouns in the first place, we would have to rely on other sources in order to do it.
	
	\section{Conclusions}
	Looking at all our results, we ended up with a pipeline that would allow us to achieve our objective - to measure the \textit{sentimental impact} of an event on a Tweet dataset. Although using an automatic Tweet generator (along with manually created Tweets) greatly helped us testing and improving our implementation, for the most part it represents synthetic data. To take this project to the next level, we would need to gather enough real-world data to fully prove our methodology. Moreover, we strongly believe that following this line of thought one could use events' features to train a predictive model based on its \textit{impactfullness}.
	
	\newpage
	
	\begin{thebibliography}{}
		
		\bibitem[\protect\citename{Arrojo}2015]{arrojo2015social}
		Maria Jose Arrojo.
		\newblock 2015.
		\newblock {\em International Journal of Social Science Studies}, volume~3.
		\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
		
		\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
		Alfred~V. Aho and Jeffrey~D. Ullman.
		\newblock 1972.
		\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
		\newblock Prentice-{Hall}, Englewood Cliffs, NJ.
		
		\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
		{American Psychological Association}.
		\newblock 1983.
		\newblock {\em Publications Manual}.
		\newblock American Psychological Association, Washington, DC.
		
		\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
		{Association for Computing Machinery}.
		\newblock 1983.
		\newblock {\em Computing Reviews}, 24(11):503--512.
		
		\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
		Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
		\newblock 1981.
		\newblock Alternation.
		\newblock {\em Journal of the Association for Computing Machinery},
		28(1):114--133.
		
		\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
		Dan Gusfield.
		\newblock 1997.
		\newblock {\em Algorithms on Strings, Trees and Sequences}.
		\newblock Cambridge University Press, Cambridge, UK.
		
	\end{thebibliography}
	
\end{document}

