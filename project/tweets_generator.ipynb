{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating random Tweets\n",
    "\n",
    "This notebook uses Markov Chains models in order to generate random texts (with a maximum length equal to a Tweet).\n",
    "\n",
    "Although it can generate reasonably accurate results, it's a completely synthetic dataset! Because of this, instead of using the generated data as training data for our NER model, we simply use it to study which text operation can improve its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating Markov Chains model\n",
    "\n",
    "Here we simply provide a series of raw text files in order to create our model - which we then save as a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markovify\n",
    "import glob\n",
    "\n",
    "max_tweet_size = 140\n",
    "raw_text_files_path = 'data/raw/markov_text_files/*.txt'\n",
    "parsed_model_file_path = 'data/parsed/markov_text_files/markov_weighted_chain.json'\n",
    "raw_text_file_paths = glob.glob(raw_text_files_path)\n",
    "raw_text_markov_models = []\n",
    "\n",
    "# Read raw text files and generate Markov chain models\n",
    "weights = []\n",
    "for file_path in raw_text_file_paths:\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        markov_model = markovify.Text(text, state_size=4)\n",
    "        raw_text_markov_models.append(markov_model)\n",
    "        if 'religion' in file_path:\n",
    "            weights.append(2)\n",
    "        else:\n",
    "            weights.append(1)\n",
    "    \n",
    "# Combine all generated models into a single one\n",
    "markov_model = markovify.combine(raw_text_markov_models, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encouraged by duty-free access to the United States.\n",
      "Everyone was pleased to see that he was deeply moved.\n",
      "July 20, 2009 Latest updates include changes to the chief of state and the premier is the head of government.\n",
      "He realized that it was a ship.\n",
      "Puncture with a tenotomy knife and scraping the interior, and the injection of new issues like the trusts and labor problems.\n"
     ]
    }
   ],
   "source": [
    "# Generate 5 random sentences from the generated Markov chain model\n",
    "for i in range(5):\n",
    "    print(markov_model.make_short_sentence(max_tweet_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model as JSON\n",
    "model_json = markov_model.chain.to_json()\n",
    "with open(parsed_model_file_path, 'w') as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving the model\n",
    "\n",
    "Even though we want to generate completely random Tweet-like texts, our aim is to improve the accuracy of our NER model for Country/Nationality/Religion/Currency recognition. Having said that, we will go through our model, generate a set amount of samples and use them as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define target datasets' paths\n",
    "parsed_country_nationality_file = 'data/parsed/parsed_country_nationality.csv'\n",
    "parsed_currency_country_file = 'data/parsed/parsed_currency_country.csv'\n",
    "parsed_country_religion_file = 'data/parsed/country_religion_files/parsed_country_religion.csv'\n",
    "parsed_country_cities_file = 'data/parsed/parsed_country_cities.csv'\n",
    "\n",
    "# Load the necessary datasets\n",
    "country_nationality_df = pd.read_csv(parsed_country_nationality_file, encoding='utf-8', compression='gzip', index_col=False)\n",
    "currency_country_df = pd.read_csv(parsed_currency_country_file, encoding='utf-8', compression='gzip', index_col=False)\n",
    "country_religion_df = pd.read_csv(parsed_country_religion_file, encoding='utf-8', compression='gzip', index_col=False)\n",
    "country_cities_df = pd.read_csv(parsed_country_cities_file, encoding='utf-8', compression='gzip', index_col=False)\n",
    "\n",
    "# Store unique sets\n",
    "unique_country_common_names = country_nationality_df['Common Name'].astype(str).unique()\n",
    "unique_country_official_names = country_nationality_df['Official Name'].astype(str).unique()\n",
    "unique_country_nationalities = country_nationality_df['Nationality'].astype(str).unique()\n",
    "unique_country_religions_name = country_religion_df['Religion'].astype(str).unique()\n",
    "unique_country_rilogions_affiliation = country_religion_df['Affiliation'].astype(str).unique()\n",
    "unique_currency_ids = currency_country_df['ID'].astype(str).unique()\n",
    "unique_country_city_names = country_cities_df['City'].astype(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "# Define important word sets\n",
    "important_word_dict = {\n",
    "    'Country Names': list(map(lambda x : x.upper(), unique_country_common_names)),\n",
    "    'Country Names (Official)': list(map(lambda x : x.upper(), unique_country_official_names)),\n",
    "    'Country Nationalities': list(map(lambda x : x.upper(), unique_country_nationalities)),\n",
    "    'Religion Names': list(map(lambda x : x.upper(), unique_country_religions_name)),\n",
    "    'Religion Affiliations': list(map(lambda x : x.upper(), unique_country_rilogions_affiliation)),\n",
    "    'Currencies': list(map(lambda x : x.upper(), unique_currency_ids)),\n",
    "    'City Names': list(map(lambda x : x.upper(), unique_country_city_names))\n",
    "}\n",
    "\n",
    "def get_word_label(word):\n",
    "    '''\n",
    "    This method checks wether or not a word\n",
    "    is considered to be 'important'.\n",
    "    '''\n",
    "    # Set regex for word parsing\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    \n",
    "    for important_word_label, important_word_set in important_word_dict.items():\n",
    "        comparable_word = regex.sub('', word).upper()\n",
    "        if comparable_word in important_word_set:\n",
    "            return important_word_label\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testing_samples_dict(max_testing_samples, unnecessary_sample_retention_percent):\n",
    "    # Define number of sentences to generate\n",
    "    cur_testing_samples = 0\n",
    "\n",
    "    testing_sample_dict = {}\n",
    "    while len(testing_sample_dict.keys()) < max_testing_samples:\n",
    "        batch_size = max_testing_samples - cur_testing_samples\n",
    "        samples = [markov_model.make_short_sentence(max_tweet_size) for i in range(batch_size)]\n",
    "        \n",
    "        for sample in samples:\n",
    "            important_words_dict = {}\n",
    "            contains_important_word = False\n",
    "            if sample is None:\n",
    "                continue\n",
    "            for word in sample.split():\n",
    "                word_label = get_word_label(word)\n",
    "                if word_label is not None:\n",
    "                    important_words_dict[word_label] = important_words_dict.get(word_label, list()) + [word]\n",
    "                    contains_important_word = True\n",
    "            if (contains_important_word):\n",
    "                print('\\n---Sample----------')\n",
    "                print('| [Text]')\n",
    "                print('| \\t{}'.format(sample))\n",
    "                print('| [Analysis results]')\n",
    "                for label, word_list in important_words_dict.items():\n",
    "                    print('| \\t{}: {}'.format(label, word_list))\n",
    "                print('| [Verification]')\n",
    "                n_city_names = int(input('| \\t[1/6] # City Names: '))\n",
    "                n_country_names = int(input('| \\t[2/6] # Country Names: '))\n",
    "                n_country_nationalities = int(input('| \\t[3/6] # Nationalities: '))\n",
    "                n_religion_names = int(input('| \\t[4/6] # Religion names: '))\n",
    "                n_religion_affiliations = int(input('| \\t[5/6] # Religious affiliations: '))\n",
    "                n_currency_names = int(input('| \\t[6/6] # Currency names: '))\n",
    "                \n",
    "                n_param_sum = n_city_names + n_country_names + n_country_nationalities + n_religion_names + n_religion_affiliations + n_currency_names\n",
    "                if (n_param_sum > 0 or random.randint(0,100) <= unnecessary_sample_retention_percent):\n",
    "                    testing_sample_dict[cur_testing_samples] = {\n",
    "                        'Text': sample,\n",
    "                        'City Names': n_city_names,\n",
    "                        'Country Names': n_country_names,\n",
    "                        'Country Nationalities': n_country_nationalities,\n",
    "                        'Religion Names': n_religion_names,\n",
    "                        'Religion Affiliations': n_religion_affiliations,\n",
    "                        'Currency Names': n_currency_names\n",
    "                    }\n",
    "                    cur_testing_samples += 1\n",
    "                    print('| [Result]')\n",
    "                    print('| \\tSAVED ({}/{})'.format(cur_testing_samples, max_testing_samples))\n",
    "                else:\n",
    "                    print('| [Result]')\n",
    "                    print('| \\tDISCARDED')\n",
    "                print('------------------')\n",
    "    return testing_sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---Sample----------\n",
      "| [Text]\n",
      "| \tThe house stood cold and silent, as if quite regardless of who had come to see whether they were getting up.\n",
      "| [Analysis results]\n",
      "| \tCity Names: ['as', 'of', 'come']\n",
      "| [Verification]\n"
     ]
    }
   ],
   "source": [
    "sample_dict = generate_testing_samples_dict(20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.DataFrame.from_dict(sample_dict, orient='index')\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define file path for output\n",
    "test_samples_file = 'data/parsed/markov_text_files/test_samples.csv'\n",
    "\n",
    "sample_df.to_csv(test_samples_file, encoding='utf-8', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
