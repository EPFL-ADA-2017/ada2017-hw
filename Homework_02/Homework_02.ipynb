{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data from the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will extract interesting information from www.topuniversities.com and www.timeshighereducation.com, two platforms that maintain a global ranking of worldwide universities. This ranking is not offered as a downloadable dataset, so you will have to find a way to scrape the information we need! You are not allowed to download manually the entire ranking -- rather you have to understand how the server loads it in your browser. For this task, Postman with the Interceptor extension can help you greatly. We recommend that you watch this brief tutorial to understand quickly how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constans definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QS_RANKING_URL = 'https://www.topuniversities.com/university-rankings/world-university-rankings/2018'\n",
    "QS_RANKING_JSON = 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt?_=1508104120137'\n",
    "\n",
    "TIMES_RANKING_URL = 'http://timeshighereducation.com/world-university-rankings/2018/world-ranking'\n",
    "TIMES_RANKING_JSON = 'https://www.timeshighereducation.com/sites/default/files/the_data_rankings/world_university_rankings_2018_limit0_369a9045a203e176392b9fb8f8c1cb2a.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEARCH_REFERENCE_API = 'https://en.wikipedia.org/w/api.php?action=query&titles={0}&prop=revisions&rvprop=content&format=json&indexpageids'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    COUNTRY_REGION_METADATA = pd.read_pickle('serial/country_region_metadata.p')\n",
    "except (OSError, IOError) as e:\n",
    "    COUNTRY_REGION_METADATA = pd.DataFrame(columns=['Region'])\n",
    "    COUNTRY_REGION_METADATA.to_pickle('serial/country_region_metadata.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_REGION_METADATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General use functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_html_parser(url):\n",
    "    '''\n",
    "    Function to build a parser object of type BeautifulSoup\n",
    "    \n",
    "    url      the webpage url to which send a get request to\n",
    "    \n",
    "    return   a parser of the given webpage\n",
    "    '''\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    page_body = r.text\n",
    "    \n",
    "    soup = BeautifulSoup(page_body, 'html.parser')\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str_number(str_n):\n",
    "    \n",
    "    return str_n.strip('\\n').strip('%').replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_standard_name(str_name):\n",
    "    \n",
    "    str_name = str_name.split(\"-\")[0] # Manage name with - (short)\n",
    "    str_name = str_name.split(\"–\")[0] # Manage name with – (long)\n",
    "    str_name = re.sub('\\(.*?\\)','', str_name) # no brackets\n",
    "    str_name = str_name.strip().replace('&', '%26')\n",
    "    \n",
    "    #r = requests.get(SEARCH_REFERENCE_API.format(str_name.strip().replace(' ', '%20')))\n",
    "    r = requests.get(SEARCH_REFERENCE_API.format(str_name.strip().replace(' ', '_')))\n",
    "    data = r.json()\n",
    "    \n",
    "    page_id = data['query']['pageids'][0]\n",
    "    \n",
    "    if (page_id == '-1'):\n",
    "        print('Not found :( -> {}'.format(str_name))\n",
    "        \n",
    "        # Manually set a standard name for the only unmatchable university. \n",
    "        # We have a total of 9 unknown sources during the WikiData requests, but only one university appears twice\n",
    "        # and it need to receive a standard name to be merged later on. The other onces could keep their name\n",
    "        if (str_name == \"Scuola Superiore Sant'Anna Pisa di Studi Universitari e di Perfezionamento\"):\n",
    "            found_name = \"Scuola Superiore Sant’Anna\"\n",
    "        else:\n",
    "            found_name = str_name\n",
    "    else:\n",
    "        found_name = data['query']['pages'][page_id]['title']\n",
    "        \n",
    "    return(found_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_country_region_metadata(country, region):\n",
    "    \n",
    "    global COUNTRY_REGION_METADATA\n",
    "    \n",
    "    if (country in COUNTRY_REGION_METADATA.index):\n",
    "        return\n",
    "    \n",
    "    new_row = pd.Series(region, index=['Region'])\n",
    "    new_row.name = country\n",
    "    \n",
    "    COUNTRY_REGION_METADATA = COUNTRY_REGION_METADATA.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Obtain the 200 top-ranking universities in www.topuniversities.com (ranking 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Handled the returning value for data not found. \n",
    "# Atm I return a -1, but this fucks up the plotting and computation for the ratios\n",
    "\n",
    "def parse_detail_page(url_detail):\n",
    "    '''\n",
    "    Function that parses the missing informations from the detail page of the university from the QS website\n",
    "    \n",
    "    Return   a dictionary with all the data found as integers values\n",
    "    '''\n",
    "    \n",
    "    # Build a parser for the detail page\n",
    "    soup = build_html_parser(url_detail)\n",
    "    \n",
    "    # Obtain and clean up the total faculty member value\n",
    "    try:\n",
    "        faculty_member_total = soup.find('div', class_='total faculty').find('div', class_='number').text\n",
    "        faculty_member_total = clean_str_number(faculty_member_total)\n",
    "    except:\n",
    "        faculty_member_total = -1\n",
    "    \n",
    "    \n",
    "    # Obtain and clean up the international faculty member value\n",
    "    try:\n",
    "        faculty_member_inter = soup.find('div', class_='inter faculty').find('div', class_='number').text.strip('\\n')\n",
    "        faculty_member_inter = clean_str_number(faculty_member_inter)\n",
    "    except:\n",
    "        faculty_member_inter = -1\n",
    "    \n",
    "    # Obtain and clean up the total students value\n",
    "    try:\n",
    "        student_total = soup.find('div', class_='total student').find('div', class_='number').text.strip('\\n')\n",
    "        student_total = clean_str_number(student_total)\n",
    "    except:\n",
    "        student_total = -1\n",
    "    \n",
    "    # Obtain and clean up the international students value\n",
    "    try:\n",
    "        student_inter = soup.find('div', class_='total inter').find('div', class_='number').text.strip('\\n')\n",
    "        student_inter = clean_str_number(student_inter)\n",
    "    except:\n",
    "        student_inter = -1\n",
    "    \n",
    "    # Build a dictionary for the parsed informations\n",
    "    detail_info = {'Total faculty member' : int(faculty_member_total), \n",
    "                   'International faculty member' : int(faculty_member_inter), \n",
    "                   'Total student' : int(student_total), \n",
    "                   'International student' : int(student_inter)\n",
    "                  }\n",
    "    \n",
    "    return detail_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some work on the Postman Inspector, we found out that the GET Request made to the QS website ended up with multiple attached files to go with the response. One of those files was a JSON with all the infos from the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "req = requests.get(QS_RANKING_JSON)\n",
    "data_from_url = req.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such data is stored as a list of dictionaries, as visible in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('First cell:')\n",
    "print(data_from_url['data'][0], end='\\n\\n')\n",
    "\n",
    "print('Second cell:')\n",
    "print(data_from_url['data'][1], end='\\n\\n')\n",
    "\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_qs_ranking():\n",
    "    '''\n",
    "    Obtain the ranking from QS in a dataframe\n",
    "    '''\n",
    "    \n",
    "    r = requests.get(QS_RANKING_JSON)\n",
    "    data = r.json()\n",
    "    \n",
    "    university_list = []\n",
    "\n",
    "    # Iterate throu the first 200 elments of the list\n",
    "    for d in data['data'][:200]:\n",
    "    \n",
    "        # Store the parsed information into a dictionary\n",
    "        info = {'Rank': d['rank_display'], \n",
    "                'University name': search_standard_name(d['title']), \n",
    "                'Country': d['country'],\n",
    "                'Region' : d['region']\n",
    "               }\n",
    "    \n",
    "        update_country_region_metadata(d['country'], d['region'])\n",
    "    \n",
    "        # Extend the dictionary with the informations in the detail page\n",
    "        url_detail = 'https://www.topuniversities.com' +  d['url']\n",
    "        info.update( parse_detail_page( url_detail))\n",
    "    \n",
    "        university_list.append(info)\n",
    "    \n",
    "    # After scraping data from QS ranking the metadata dataframe needs to be stored updated\n",
    "    COUNTRY_REGION_METADATA.to_pickle('serial/country_region_metadata.p')\n",
    "    \n",
    "    qs_ranking_df = pd.DataFrame.from_dict(university_list)\n",
    "    return qs_ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    qs_ranking_df = pd.read_pickle('serial/qs_save.p')\n",
    "except (OSError, IOError) as e:\n",
    "    qs_ranking_df = scrape_qs_ranking()\n",
    "    qs_ranking_df.to_pickle('serial/qs_save.p')\n",
    "    \n",
    "qs_ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs_ranking_df.set_index(['University name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the two required ratios with the help of two auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_facutly_member_ratio(df):\n",
    "    '''\n",
    "    Co\n",
    "    '''\n",
    "    li = list()\n",
    "    for i, row in df.iterrows():\n",
    "        li.append(row['Total faculty member'] / row['Total student'])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_student_ratio(df):\n",
    "    li = list()\n",
    "    for i, row in df.iterrows():\n",
    "        li.append(row['International student'] / row['Total student'])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation results are stored in two new colums of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qs_ranking_df['Faculty/students ratio'] = compute_facutly_member_ratio( qs_ranking_df )\n",
    "qs_ranking_df['Intern/student ratio'] = compute_student_ratio( qs_ranking_df )\n",
    "\n",
    "qs_ranking_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the dataframe for the ratios computed (double click on the plot for zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "qs_ranking_df[['Faculty/students ratio', 'Intern/student ratio']].plot(kind='barh', figsize=(10,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results aggregating by region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (title, group) in enumerate(qs_ranking_df.groupby('Region')):\n",
    "    ax = group[['Faculty/students ratio', 'Intern/student ratio']].plot.bar(figsize=(17, 7), \n",
    "                                                                            width= 0.5 if (len(group) > 2) else 0.1)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation = 90 if (len(group) > 5) else 0)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results aggregating by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (title, group) in enumerate(qs_ranking_df.groupby('Country')):\n",
    "    ax = group[['Faculty/students ratio', 'Intern/student ratio']].plot.bar(figsize=(17, 7), \n",
    "                                                                            width= 0.5 if (len(group) > 5) else 0.1)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation = 90 if (len(group) > 5) else 0)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Scrape the Times ranking\n",
    "Obtain the 200 top-ranking universities in www.timeshighereducation.com (ranking 2018). Repeat the analysis of the previous point and discuss briefly what you observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_value_from_percentage(total, percentage):\n",
    "    \n",
    "    total = int( total )\n",
    "    percentage = float( percentage )\n",
    "    \n",
    "    return round( (total/100) * percentage )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_value_from_proportion(total, proportion):\n",
    "    \n",
    "    total = int( total )\n",
    "    proportion = float( proportion )\n",
    "    \n",
    "    return round( total / proportion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_times_ranking():\n",
    "    '''\n",
    "    Obtain the ranking from Times in a dataframe\n",
    "    '''\n",
    "\n",
    "    r = requests.get(TIMES_RANKING_JSON)\n",
    "    data = r.json()\n",
    "\n",
    "    university_list = []\n",
    "\n",
    "    # Iterate throu the first 200 elments of the list\n",
    "    for d in data['data'][:200]:\n",
    "    \n",
    "        # Preliminary computations to extract data\n",
    "        intern_student = compute_value_from_percentage( clean_str_number( d['stats_number_students']), \n",
    "                                                       clean_str_number( d['stats_pc_intl_students'])\n",
    "                                                      )\n",
    "    \n",
    "        faculty_member_total = compute_value_from_proportion( clean_str_number( d['stats_number_students']), \n",
    "                                                             clean_str_number( d['stats_student_staff_ratio'])\n",
    "                                                            )\n",
    "        \n",
    "        # Determine region from the data of the QS ranking stored in the metadata\n",
    "        try:\n",
    "            region = COUNTRY_REGION_METADATA.get_value(d['location'], 'Region')\n",
    "        except:\n",
    "            region = 'NaN'\n",
    "    \n",
    "        # Store the parsed information into a dictionary\n",
    "        info = {'Rank': d['rank'], \n",
    "                'University name': search_standard_name(d['name']), \n",
    "                'Country': d['location'],\n",
    "                'Region' : region,\n",
    "                'Total student' : int(clean_str_number( d['stats_number_students'])),\n",
    "                'International student' : int(intern_student),\n",
    "                'Total faculty member' : int(faculty_member_total)\n",
    "               }\n",
    "    \n",
    "        university_list.append(info)\n",
    "   \n",
    "    times_ranking_df = pd.DataFrame.from_dict(university_list)\n",
    "    return times_ranking_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    times_ranking_df = pd.read_pickle('serial/times_save.p')\n",
    "except (OSError, IOError) as e:\n",
    "    times_ranking_df = scrape_times_ranking()\n",
    "    times_ranking_df.to_pickle('serial/times_save.p')\n",
    "    \n",
    "times_ranking_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missig data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_ranking_df.set_index(['University name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#times_ranking_df['Faculty/students ratio'] = compute_facutly_member_ratio( qs_ranking_df )\n",
    "times_ranking_df['Intern/student ratio'] = compute_student_ratio( times_ranking_df )\n",
    "\n",
    "times_ranking_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "times_ranking_df.sort_values('Intern/student ratio')[['Intern/student ratio']].plot( kind='barh', figsize=(10,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results aggregating by region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (title, group) in enumerate(times_ranking_df.groupby('Region')):\n",
    "    ax = group[['Intern/student ratio']].plot.bar(figsize=(17, 7), \n",
    "                                                                            width= 0.5 if (len(group) > 2) else 0.1)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation = 90 if (len(group) > 5) else 0)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results aggregating by country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (title, group) in enumerate(times_ranking_df.groupby('Country')):\n",
    "    ax = group[['Intern/student ratio']].plot.bar(figsize=(17, 7), \n",
    "                                                                            width= 0.5 if (len(group) > 5) else 0.1)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation = 90 if (len(group) > 5) else 0)\n",
    "    plt.xlabel(\"\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "source": [
    "### Task 3 - Merge the dataframes\n",
    "Merge the two DataFrames created in questions 1 and 2 using university names. Match universities' names as well as you can, and explain your strategy. Keep track of the original position in both rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_ranking_df['University name'] = qs_ranking_df.index\n",
    "times_ranking_df['University name'] = times_ranking_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_ranking_df['University name'] = qs_ranking_df['University name'].str.strip()\n",
    "qs_ranking_df['Country'] = qs_ranking_df['Country'].str.strip()\n",
    "\n",
    "times_ranking_df['University name'] = times_ranking_df['University name'].str.strip()\n",
    "times_ranking_df['Country'] = times_ranking_df['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the two dataframes into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "merged_ranking_df = pd.merge(qs_ranking_df, times_ranking_df, \n",
    "                             on=['University name', 'Country', 'Region'], \n",
    "                             how='outer', \n",
    "                             suffixes=('_QS', '_TM')\n",
    "                            )\n",
    "\n",
    "merged_ranking_df.set_index(['University name'], inplace=True)\n",
    "\n",
    "merged_ranking_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
