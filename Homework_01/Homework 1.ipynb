{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RELEVANT FAQs SOLVED BY TAs**\n",
    "\n",
    "* In Task 3, by \"transformation\" of categorical values, do you mean getting dummies for all of them? This does'’ play well for bar charts. ***Good idea.***\n",
    " \n",
    "* Task two asks for a unique index for the final data frame. Is it sufficient to give the data frame a numerical index from 0 - > X? Or should the index be made up of a combination of the different attributes? ***Yes, it has to be a meaningful index.***\n",
    "\n",
    "* How is the “cabin floor” defined? (see homework 1, task 3.3) If cabin letter indicates the floor, what about the guy in cabin “F E46”? Also the “T” seems like a typo. There were 5 such cases..And what about people who reserved more cabins. **Argue about your decisions; Real world datasets are dirty! (Useful: https://www.encyclopedia-titanica.org/titanic-deckplans)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from calendar import monthrange\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_NAME_COLUMN_DATE = 'Date'\n",
    "DEFAULT_NAME_COLUMN_DESCRIPTION = 'Description'\n",
    "DEFAULT_NAME_COLUMN_TOTAL = 'Totals'\n",
    "DEFAULT_NAME_COLUMN_COUNTRY = 'Country'\n",
    "\n",
    "DEFAULT_NAME_ROW_DESCRIPTION_NEW_CASES = 'New cases'\n",
    "DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS = 'New deaths'\n",
    "\n",
    "DEFAULT_DATA_FORMAT = '%Y-%m-%d'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The definitions of the basic schema and formats for the given dataset are stored in the following dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "country_keys = ['guinea', 'liberia', 'sl']\n",
    "paths = {x: DATA_FOLDER + '/ebola/'+ x + '_data' for x in country_keys}\n",
    "countries = {'guinea': 'Guinea', 'liberia': 'Liberia', 'sl': 'Sierra Leone'}\n",
    "\n",
    "wanted_columns = {'guinea': ['Date', 'Description', 'Totals'],\n",
    "                      'liberia': ['Date', 'Variable', 'National'],\n",
    "                      'sl': ['date', 'variable', 'National']}\n",
    "\n",
    "date_original_formats = {'guinea': ['%Y-%m-%d', '%y-%m-%d'],\n",
    "                         'liberia': ['%m/%d/%Y', '%m/%d/%y'],\n",
    "                         'sl': ['%Y-%m-%d', '%y-%m-%d']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sanitize_date(date, original_formats):\n",
    "    '''\n",
    "    Transform a date specified in a string with a certain original\n",
    "    format into a string with the date in the default format.\n",
    "    '''\n",
    "    date_rep = None\n",
    "    try:\n",
    "        date_rep = datetime.datetime.strptime(date, original_formats[0])\n",
    "    except ValueError:\n",
    "        date_rep = datetime.datetime.strptime(date, original_formats[1])\n",
    "    return date_rep.strftime(DEFAULT_DATA_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_daily_from_cum(np_list):\n",
    "    '''\n",
    "    Given a list of accumulated data, calculate the data for each day.\n",
    "    It assumes ordered data.\n",
    "    '''\n",
    "    last_known_n = -1\n",
    "    final_list = list()\n",
    "    for i, elem in np_list.iteritems():\n",
    "        #elem = float(elem)\n",
    "        if (math.isnan(elem)):\n",
    "            final_list.append(0)\n",
    "        else:\n",
    "            if (last_known_n == -1):\n",
    "                final_list.append(0)\n",
    "            else:\n",
    "                final_list.append(elem - last_known_n)\n",
    "            last_known_n = elem\n",
    "    return np.array(final_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary `dataframe_list` that contains, for each country, the corresponding dataframe.\n",
    "\n",
    "After reading each data frame we apply the function `sanitize_date` to the `Date` column to make sure that the format used for the date is consistent, and we then sort the rows in each dataframe according to the `Date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_list = {}\n",
    "\n",
    "for i in country_keys:\n",
    "    temp_list = []\n",
    "    all_files = glob.glob(paths[i] + '/*.csv')\n",
    "    for file in all_files:\n",
    "        temp_df = pd.read_csv(file)\n",
    "        temp_list.append(temp_df)\n",
    "    dataframe_list[i] = pd.concat(temp_list)\n",
    "    \n",
    "    # Format dates and order dataframes by date\n",
    "    dataframe_list[i][wanted_columns[i][0]] = np.array([sanitize_date(x, date_original_formats[i])\n",
    "                                                        for x in dataframe_list[i][wanted_columns[i][0]]])\n",
    "    \n",
    "    dataframe_list[i][wanted_columns[i][0]] = pd.to_datetime(dataframe_list[i][wanted_columns[i][0]])\n",
    "    dataframe_list[i].sort_values(by=wanted_columns[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Parsing data for each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the parsing processes is to obtain a *standard* datataset for each country. We want to extract the useful data for our task while reorganizing the schema and data format to develop a general coherence.\n",
    "\n",
    "In particular, we want to obtain the daily number of deaths and new cases registered for each country.\n",
    "\n",
    "***N.B.*** The resulting dataframes do not have the indexing reformatted. Since those are temporary results that will later be concatenated in a single dataframe, we decided to handle the indexing after this operation.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Guinea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse the Guinea dataset we considered the variables ```New deaths registered``` (or ```New deaths registered today``` when it was used instead of the previous one) and ```Total new cases registered so far```. These variables represent the daily number of deaths and cases respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select certain column from the CSV formatted files\n",
    "parsed_guinea_df = dataframe_list['guinea'][wanted_columns['guinea']]\n",
    "guinea_wanted_row_list = ['Total new cases registered so far',\n",
    "                          'New deaths registered',\n",
    "                          'New deaths registered today']\n",
    "\n",
    "# Select all the rows that match desired names for 'Description' column\n",
    "parsed_guinea_df = parsed_guinea_df[parsed_guinea_df['Description'].isin(guinea_wanted_row_list)]\n",
    "\n",
    "# Rename every row value for 'Description' column \n",
    "parsed_guinea_df.loc[parsed_guinea_df['Description'] ==\n",
    "                     guinea_wanted_row_list[0], 'Description'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_CASES\n",
    "\n",
    "parsed_guinea_df.loc[parsed_guinea_df['Description'] ==\n",
    "                     guinea_wanted_row_list[1], 'Description'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS\n",
    "\n",
    "parsed_guinea_df.loc[parsed_guinea_df['Description'] ==\n",
    "                     guinea_wanted_row_list[2], 'Description'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS\n",
    "\n",
    "# Add 'Country' column to dataframe\n",
    "parsed_guinea_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['guinea']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_guinea_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Liberia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse the Liberia dataset we considered the ```Newly reported deaths``` variable, which represents the daily number of deaths. To handle the new cases data we, on the other hand, had to do some manipulations. \n",
    "\n",
    "The number of new (daily) cases comes in three different variables (```New Case/s (Suspected)```, ```New Case/s (Probable)``` and ```New Case/s (confirmed)```) that have to be summed up to obtain the total value of new cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select certain columns from the CSV formatted files\n",
    "liberia_df = dataframe_list['liberia'][wanted_columns['liberia']]\n",
    "\n",
    "liberia_wanted_row_list = ['New Case/s (Suspected)',\n",
    "                   'New Case/s (Probable)',\n",
    "                   'New case/s (confirmed)',\n",
    "                   'Newly reported deaths',\n",
    "                   'Total suspected cases',\n",
    "                   'Total probable cases',\n",
    "                   'Total confirmed cases']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the data comes with strangely big new cases values for the last days of the report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liberia_df[(liberia_df['Variable'] == 'New case/s (confirmed)') &\n",
    "           (liberia_df['Date'].apply(lambda x: x.month) == 12)].sort_values(by=['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concluded that those numbers couldn't be correct. As we found a correlation between those values and the total new cases accumulator for the past days, we assumed the data as been inputted on the wrong part of the table.\n",
    "\n",
    "The new cases data for the last month of the report were therefore calculated from the daily difference of the cases accumulators, while we took the daily values for the rest of the months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary dataframes for new case entries\n",
    "# Take accumulated cases for the last month of the report\n",
    "liberia_df_cum_cases_1 = liberia_df[liberia_df['Variable'].isin(liberia_wanted_row_list[4:7])]\n",
    "liberia_df_cum_cases_1 = liberia_df_cum_cases_1[liberia_df_cum_cases_1['Date'].map(\n",
    "    lambda x: x.month == 12 and x.day <= 3)]\n",
    "liberia_df_cum_cases_1 = liberia_df_cum_cases_1.groupby('Date').sum()\n",
    "\n",
    "liberia_df_cum_cases_2 = liberia_df[liberia_df['Variable'].isin(liberia_wanted_row_list[0:3])]\n",
    "liberia_df_cum_cases_2 = liberia_df_cum_cases_2[liberia_df_cum_cases_2['Date'].map(\n",
    "    lambda x: x.month == 12 and x.day > 3)]\n",
    "liberia_df_cum_cases_2 = liberia_df_cum_cases_2.groupby('Date').sum()\n",
    "\n",
    "liberia_df_cum_cases = pd.concat([liberia_df_cum_cases_1, liberia_df_cum_cases_2])\n",
    "liberia_df_cum_cases['National'] = estimate_daily_from_cum(liberia_df_cum_cases['National'])\n",
    "\n",
    "# Take daily cases for the other months\n",
    "liberia_df_new_cases = liberia_df[liberia_df['Variable'].isin(liberia_wanted_row_list[0:3])]\n",
    "liberia_df_new_cases = liberia_df_new_cases[liberia_df_new_cases['Date'].map(\n",
    "    lambda x: x.month != 12)]\n",
    "\n",
    "# Get dataframe with new cases for all months\n",
    "liberia_df_new_cases = pd.concat([liberia_df_new_cases, liberia_df_cum_cases])\n",
    "\n",
    "# Sum all of the values for 'Probable', 'Variable' and 'Confirmed' new cases\n",
    "liberia_df_new_cases = liberia_df_new_cases.groupby('Date').sum()\n",
    "\n",
    "liberia_df_new_cases['Date'] = liberia_df_new_cases.index\n",
    "liberia_df_new_cases['Variable'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary dataframe for new deaths entries\n",
    "liberia_df_new_deaths = liberia_df[liberia_df['Variable'] == liberia_wanted_row_list[3]]\n",
    "liberia_df_new_deaths['Variable'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dataframe with new deaths and cases (concatenating the two temporary)\n",
    "parsed_liberia_df = pd.concat([liberia_df_new_cases, liberia_df_new_deaths])\n",
    "\n",
    "parsed_liberia_df.rename(columns={'Date': DEFAULT_NAME_COLUMN_DATE,\n",
    "                                  'Variable': DEFAULT_NAME_COLUMN_DESCRIPTION,\n",
    "                                  'National': DEFAULT_NAME_COLUMN_TOTAL}, inplace=True)\n",
    "\n",
    "# Add 'Country' column to dataframe\n",
    "parsed_liberia_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['liberia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_liberia_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.3 Sierra Leone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse the Sierra Leone dataset we followed similar steps as for Liberia. The new cases value has been computed as the sum of the corresponding variables of the table (```new_noncase```, ```new_suspected```, ```new_probable``` and ```new_confirmed```). \n",
    "\n",
    "Since the dataset only had total accumulators for the number of deaths, the new death variable was calculated as the difference between days, as previously done with Liberia. The possible missing data for this operation has been handled by setting them to zero, while keeping the the same delta for known points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select certain columns from the CSV formatted files\n",
    "parsed_sierra_df = dataframe_list['sl'][wanted_columns['sl']]\n",
    "\n",
    "sierra_wanted_row_list = ['new_noncase', 'new_suspected', 'new_probable', 'new_confirmed',\n",
    "                   'death_confirmed', 'death_probable', 'death_suspected']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary dataframe for new case entries\n",
    "# (with every row that matches the desired 'variables' column value)\n",
    "sierra_df_new_cases = parsed_sierra_df[parsed_sierra_df['variable'].isin(\n",
    "    sierra_wanted_row_list[0:4])]\n",
    "\n",
    "# Sum all of the values for 'Probable', 'Variable', 'Confirmed' and 'Noncase' new cases\n",
    "sierra_df_new_cases = sierra_df_new_cases.groupby('date')['National'].apply(\n",
    "    lambda x: np.array([float(y) for y in x]).sum()).to_frame()\n",
    "\n",
    "sierra_df_new_cases['date'] = sierra_df_new_cases.index\n",
    "sierra_df_new_cases['variable'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create temporary dataframe for new deaths' accumulator entries\n",
    "sierra_df_new_deaths_cum = parsed_sierra_df[parsed_sierra_df['variable'].isin(\n",
    "    sierra_wanted_row_list[4:7])]\n",
    "\n",
    "# Sum all of the values for 'Probable', 'Suspected' and 'Confirmed' new death accumulators\n",
    "sierra_df_new_deaths_cum = sierra_df_new_deaths_cum.groupby('date')['National'].apply(\n",
    "    lambda x: np.array([float(y) for y in x]).sum()).to_frame()\n",
    "\n",
    "sierra_df_new_deaths_cum['date'] = sierra_df_new_deaths_cum.index\n",
    "sierra_df_new_deaths_cum['variable'] = 'New deaths accumulator'\n",
    "\n",
    "# \n",
    "sierra_df_new_deaths_cum['National'] = estimate_daily_from_cum(sierra_df_new_deaths_cum['National'])\n",
    "sierra_df_new_deaths_cum['variable'] = DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_sierra_df = pd.concat([sierra_df_new_cases, sierra_df_new_deaths_cum])\n",
    "\n",
    "parsed_sierra_df.rename(columns={'date': DEFAULT_NAME_COLUMN_DATE,\n",
    "                                 'variable': DEFAULT_NAME_COLUMN_DESCRIPTION,\n",
    "                                 'National': DEFAULT_NAME_COLUMN_TOTAL}, inplace=True)\n",
    "\n",
    "# Add 'Country' column to dataframe\n",
    "parsed_sierra_df[DEFAULT_NAME_COLUMN_COUNTRY] = countries['sl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sierra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Joining the parsed country datatset into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_df = pd.concat([parsed_guinea_df, parsed_liberia_df, parsed_sierra_df])\n",
    "complete_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[42:47]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after handling the NaN troubles during the parsing for some aspects, we still have some to manage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[complete_df[DEFAULT_NAME_COLUMN_TOTAL].isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for now, we assume that the missing values equal 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume NaN values are 0\n",
    "complete_df = complete_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that the difference in the number of deaths for some rows is negative. It is not a reasonable value and is probably due to an error when keeping track of the accumulated number of deaths, especially during the change of month. We therefore decided to consider 0 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[complete_df['Totals'].apply(lambda x: int(x)) < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_df.loc[complete_df['Totals'].apply(lambda x: int(x)) < 0, 'Totals'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Calculate for each country, the daily average per month of new cases and deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start grouping by ```Country```, ```Description``` and month, and then calculate the average over the total number of days in the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = complete_df.groupby(by=[complete_df.Country, complete_df.Description,\n",
    "                                  [x.month for x in complete_df.Date]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calculated_average_df = pd.DataFrame()\n",
    "\n",
    "for ((country, description, month), values) in grouped:\n",
    "    days_in_month = monthrange(2014, month)[1]\n",
    "    entry = {DEFAULT_NAME_COLUMN_COUNTRY: country, \n",
    "             'Description': description, \n",
    "             'Month': month, \n",
    "             'Average': values[DEFAULT_NAME_COLUMN_TOTAL].apply(lambda x: float(x) / days_in_month).sum()}\n",
    "    calculated_average_df = calculated_average_df.append([entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "death_average_df = calculated_average_df[calculated_average_df[DEFAULT_NAME_COLUMN_DESCRIPTION] ==\n",
    "                                         DEFAULT_NAME_ROW_DESCRIPTION_NEW_DEATHS]\n",
    "cases_average_df = calculated_average_df[calculated_average_df[DEFAULT_NAME_COLUMN_DESCRIPTION] ==\n",
    "                                         DEFAULT_NAME_ROW_DESCRIPTION_NEW_CASES]\n",
    "\n",
    "calculated_average_df = pd.merge(death_average_df, cases_average_df, on=[DEFAULT_NAME_COLUMN_COUNTRY, 'Month'])\n",
    "calculated_average_df.drop([DEFAULT_NAME_COLUMN_DESCRIPTION+'_x',\n",
    "                            DEFAULT_NAME_COLUMN_DESCRIPTION+'_y'], axis=1, inplace=True)\n",
    "\n",
    "calculated_average_df = calculated_average_df.rename(columns={'Average_x': 'Death monthly average',\n",
    "                                                              'Average_y': 'Case monthly average'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reordering the columns, the final result is shown in the following table. For each country we calculated the average for the new deaths entries as well as the new cases entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_average_df = calculated_average_df[['Country', 'Month',\n",
    "                                               'Death monthly average', 'Case monthly average']]\n",
    "calculated_average_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plotted the results for a final visualisation of the obtained data. First by country alone in the following bar charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 7))\n",
    "\n",
    "for i, (title, group) in enumerate(calculated_average_df.groupby(DEFAULT_NAME_COLUMN_COUNTRY)):\n",
    "    group.plot.bar(x='Month', title=title, ax=axes.flat[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lastly as an unique bar chart for all the results in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_average_df.plot.bar(x=['Month',DEFAULT_NAME_COLUMN_COUNTRY], figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We started by importing the nine tables in one single big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(DATA_FOLDER + '/microbiome' + '/MID*.xls')\n",
    "raw_df = pd.DataFrame()\n",
    "\n",
    "for file in all_files:\n",
    "    file_name = file[file.rfind('/')+1:file.rfind('.')]\n",
    "    temp_df = pd.read_excel(file, header=None)\n",
    "    temp_df['BARCODE'] = file_name\n",
    "    raw_df = raw_df.append(temp_df)\n",
    "\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the imported the metadata table in another dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_excel(DATA_FOLDER+'/microbiome'+'/metadata.xls')\n",
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Merging the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final goal is to obtain a single dataframe for the given data. We will need to obtain a table of the following format:\n",
    "\n",
    "| Description | Group | Tissue | Stool | Other |\n",
    "|-------------|-------|--------|-------|-------|\n",
    "|             |       |        |       |       |\n",
    "|             |       |        |       |       |\n",
    "|             |       |        |       |       |\n",
    "\n",
    "\n",
    "We will fill-in the last three collumns with the sample date we have in the given datatframes. The column will be chosen according to what the metadata provides. The same idea will be followed to fill-in the Group collumn with the correct information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create three dataframe containing the metadata informations for a single Sample type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stool_metadata_df = metadata_df[metadata_df['SAMPLE'] == 'stool']\n",
    "tissue_metadata_df = metadata_df[metadata_df['SAMPLE'] == 'tissue']\n",
    "na_metadata_df = metadata_df[metadata_df['SAMPLE'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create three dataframes for the different samples according to what we have in the metatada dataframes obtained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_df = pd.merge(raw_df, metadata_df[['BARCODE', 'GROUP']], on=['BARCODE'])\n",
    "raw_df.rename(columns={0: 'Description', 1: 'Sample'}, inplace=True)\n",
    "\n",
    "stool_df = raw_df[raw_df['BARCODE'].isin(list(stool_metadata_df['BARCODE']))].drop('BARCODE', axis=1)\n",
    "stool_df.rename(columns={'Sample': 'Stool'}, inplace=True)\n",
    "              \n",
    "tissue_df = raw_df[raw_df['BARCODE'].isin(list(tissue_metadata_df['BARCODE']))].drop('BARCODE', axis=1)\n",
    "tissue_df.rename(columns={'Sample': 'Tissue'}, inplace=True)\n",
    "              \n",
    "na_df = raw_df[raw_df['BARCODE'].isin(list(na_metadata_df['BARCODE']))].drop('BARCODE', axis=1)\n",
    "na_df.rename(columns={'Sample': 'Other'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stool_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge those three temporary dataframes into a single one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(stool_df, tissue_df, how='outer', on=['Description', 'GROUP'])\n",
    "merged_df = pd.merge(merged_df, na_df, how='outer', on=['Description', 'GROUP'])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the task, we fill the NaN values of the dataset with the 'unknown' value and we clear the general format of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill of the 'unknown' value for NaNs\n",
    "merged_df.fillna('unknown', inplace=True)\n",
    "\n",
    "# Final cleaning up of the table schema\n",
    "merged_df.rename(columns={'GROUP':'Group'}, inplace=True)\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OTHER POSSIBILITY FOR THIS TASK\n",
    "\n",
    "def return_from_barcode(barcode, dictionary):\n",
    "    for x in dictionary:\n",
    "        if barcode in dictionary[x]:\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "parsed_df = raw_df.copy()\n",
    "\n",
    "groups_dictionary = {}\n",
    "for group in metadata_df['GROUP'].unique():\n",
    "    groups_dictionary[group] = metadata_df[metadata_df['GROUP'] == group]['BARCODE'].values\n",
    "\n",
    "parsed_df['GROUP'] = parsed_df['BARCODE'].map(lambda x: return_from_barcode(x, groups_dictionary))\n",
    "parsed_df.rename(columns={1: 'VALUE', 0: 'GENUS'}, inplace=True)\n",
    "parsed_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "samples_dictionary = {'Nan': ['MID1']}\n",
    "for sample in metadata_df['SAMPLE'].unique():\n",
    "    samples_dictionary[sample] = metadata_df[metadata_df['SAMPLE'] == sample]['BARCODE'].values\n",
    "\n",
    "parsed_df['SAMPLE'] = parsed_df['BARCODE'].map(lambda x: return_from_barcode(x, samples_dictionary))\n",
    "\n",
    "parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df = pd.read_excel(DATA_FOLDER+'/titanic.xls')\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata_df = pd.read_html(DATA_FOLDER+'/titanic.html', header=0)\n",
    "\n",
    "# Fills the null values in the first column with the last valid value\n",
    "metadata_df[1]['Variable'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Description of attributes\n",
    "Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in metadata_df[0]['Name']:\n",
    "    if titanic_df[attribute].isnull().any():\n",
    "        print(attribute+' has null values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pclass: integer; 3 levels: 1, 2 or 3; Can be turned into an integer.\n",
    "* survived: binary, 0 means that the passenger survived and 1 means that did not survive. Equivalent to a categorical attribute with 2 levels\n",
    "* name: string\n",
    "* sex: string; 2 levels: male or female\n",
    "* age: real number; If it's less than 1 then the passenger was younger than 1 year old. If it's in the form xx.5 then the age is an estimation; (null values possible)\n",
    "* sibsp: integer\n",
    "* parch: integer\n",
    "* ticket: string, but could be reformatted to be an integer\n",
    "* fare: real number (null values possible)\n",
    "* cabin: string; 187 levels (null values possible)\n",
    "* embarked: string; 3 levels: S, C or Q (null values possible)\n",
    "* boat: string; 28 levels (null values possible)\n",
    "* body: integer (null values possible)\n",
    "* home.dest: string (null values possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones that can be categorical are:\n",
    "\n",
    "* pclass\n",
    "* survived\n",
    "* sex\n",
    "* cabin\n",
    "* embarked\n",
    "* boat\n",
    "\n",
    "All of them appear in `metadata_df[1]` except `survived`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_range_categorical = {'survived': titanic_df['survived'].unique()}\n",
    "for attribute in metadata_df[1]['Variable'].unique():\n",
    "    value_range_categorical[attribute] = np.array(metadata_df[1][metadata_df[1]['Variable'] == attribute]['Levels'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TRANSFORMATION OPTION 1\n",
    "\n",
    "titanic_df['survived'] = titanic_df['survived'].map(lambda x: 'Survived' if x else 'Died')\n",
    "titanic_df['pclass'] = titanic_df['pclass'].map(lambda x: '1st' if x==1 else '2nd' if x==2 else '3rd')\n",
    "titanic_df['embarked'] = titanic_df['embarked'].map(lambda x: 'Cherbourg' if x=='C' else 'Queenstown' if x=='Q' else 'Southampton' if x=='S' else 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRANSFORMATION OPTION 2\n",
    "# survived is already binary\n",
    "# We don't transform 'boat' because it won't be used\n",
    "# We create a variable 'cabin_floor' and then create dummy variables for it\n",
    "\n",
    "titanic_df_2 = titanic_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_df_2['cabin'].fillna(value='', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_floor(cabin):\n",
    "    # Cabin T is in the boat deck\n",
    "    # We assume cabins in the format F Gxx are in deck F (and therefore in floor 6)\n",
    "    map_dictionary = {'T': 0, 'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n",
    "    if not cabin:\n",
    "        return None\n",
    "    else:\n",
    "        cabin_list = str(cabin).split()\n",
    "        # Assumes the last cabin listed is the valid one\n",
    "        # This also makes sure that F Gxx counts as a cabin in deck F (assumption)\n",
    "        valid_cabin = cabin_list[0]\n",
    "        return map_dictionary[valid_cabin[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_df_2['cabin_floor'] = titanic_df_2['cabin'].map(lambda x: find_floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(titanic_df_2, columns=['pclass', 'sex', 'embarked','cabin_floor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Histogram plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide four histograms in the following section. Those plots visualize different amount of passengers from the dataset according to the `travel class`, `sex`, `embarked port` and `age`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Axuliary function for the parsing of the dataset to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ord_from_str(string):\n",
    "    ord_dic = {'nan':0, 'S':1, 'Q':2, 'C':3,}\n",
    "    stripped_string = str(string).strip().strip('\\n')\n",
    "    return ord_dic.get(stripped_string, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "\n",
    "# Travel class dataset and histogram setup\n",
    "travel_class_df = pd.DataFrame(titanic_df[['pclass']])\n",
    "travel_class_df.plot.hist(ax=axes[0,0])\n",
    "\n",
    "plt.sca(axes[0, 0])\n",
    "plt.title('Passengers by class')\n",
    "plt.legend(loc='upper center')\n",
    "plt.sca(axes[0, 0])\n",
    "plt.xticks(np.arange(1.0, 4.0, 1.0), ('1st class', '2nd class', '3rd class'))\n",
    "\n",
    "\n",
    "# Sex dataset and histogram setup\n",
    "sex_df = pd.DataFrame(titanic_df[['sex']])\n",
    "sex_df['sex'] = sex_df['sex'].apply(lambda x : 1 if x == 'male' else 0) #mapping \n",
    "sex_df.plot.hist(ax=axes[0,1])\n",
    "\n",
    "plt.sca(axes[0, 1])\n",
    "plt.title('Passengers by sex')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xticks(np.arange(0.0, 2.0, 1.0), ('female', 'male'))\n",
    "\n",
    "\n",
    "# Embarked port dataset and histogram setup\n",
    "embarked_df = pd.DataFrame(titanic_df[['embarked']])\n",
    "embarked_df = embarked_df.apply(lambda x: x.apply(lambda y: get_ord_from_str(y)))\n",
    "embarked_df.plot.hist(ax=axes[1,0])\n",
    "\n",
    "plt.sca(axes[1, 0])\n",
    "plt.title('Passengers by port')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(0.0, 4.0, 1.0), ('NaN', 'Southampton', 'Queenstown', 'Cherbourg'))\n",
    "\n",
    "\n",
    "# Age dataset and histogram setup\n",
    "age_df = pd.DataFrame(titanic_df[['age']])\n",
    "age_df.plot.hist(ax=axes[1,1])\n",
    "\n",
    "plt.sca(axes[1, 1])\n",
    "plt.title('Passengers by age')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xticks(np.arange(0.0, 100.0, 10.0))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Passengers by cabin floor\n",
    "Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ax = sns.countplot(x='cabin_floor', data=titanic_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passengers_per_cabin_floor = {}\n",
    "for floor in titanic_df_2['cabin_floor'].unique():\n",
    "    if str(floor) != 'nan':\n",
    "        passengers_per_cabin_floor[int(floor)] = titanic_df_2[titanic_df_2['cabin_floor'] == floor]['cabin_floor'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "plt.pie(list(passengers_per_cabin_floor.values()), labels=passengers_per_cabin_floor.keys(), autopct='%1.0f%%')\n",
    "plt.axis('equal')\n",
    "plt.title('Passengers per floor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Survivors per class\n",
    "For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survivors_per_class_df = titanic_df[['pclass', 'survived']].groupby(['pclass', 'survived']).agg(len)\n",
    "\n",
    "# Plot a pie chart for each class\n",
    "for pclass in range(1,4):\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.pie(survivors_per_class_df[pclass], labels=['Died', 'Survived'], autopct='%1.0f%%')\n",
    "    plt.axis('equal')\n",
    "    plt.title('Class '+str(pclass))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Survivors per class and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for survived entries on their class and sex atributes\n",
    "survivors_per_class_sex_df = pd.DataFrame(titanic_df[titanic_df.survived == 1][['pclass', 'sex']])\n",
    "\n",
    "# Split of the data in two datasets (one for male, one for female)\n",
    "male_df = survivors_per_class_sex_df[survivors_per_class_sex_df['sex'] == 'male']\n",
    "female_df = survivors_per_class_sex_df[survivors_per_class_sex_df['sex'] != 'male']\n",
    "\n",
    "# Plotting of the two datasets as stacked histogram\n",
    "fig = plt.figure(figsize=(15,7))\n",
    "plt.hist([male_df['pclass'], female_df['pclass']], label=['male', 'female'], stacked=True)\n",
    "plt.title('Survivors by class')\n",
    "plt.legend(loc='upper center')\n",
    "plt.xticks(np.arange(1.0, 4.0, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Age categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to define the `age category` in the following way: we took the median age value and then grouped all the people younger in category 1 and all the older ones in category 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the age category\n",
    "titanic_df['age_category'] = titanic_df['age'].map(lambda x: 1 if x <= titanic_df['age'].median() \n",
    "                                                                   else 2 if x > titanic_df['age'].median() \n",
    "                                                                             else None)\n",
    "\n",
    "age_categories_df = pd.DataFrame(titanic_df[titanic_df.survived == 1][['age_category', 'pclass', 'sex']]\n",
    "                                 .groupby(['age_category', 'pclass', 'sex'])\n",
    "                                 .agg(len))\n",
    "# Compute the proportion ratio\n",
    "age_categories_df = age_categories_df.apply(lambda x : x / age_categories_df.values.sum())\n",
    "\n",
    "# Re indexing of the table\n",
    "age_categories_df.reset_index(inplace=True)\n",
    "\n",
    "# Renaming of the datatset schema\n",
    "age_categories_df.rename(columns={'age_category':'age category', 'pclass':'class', 0:'proportion'}, inplace=True)\n",
    "\n",
    "age_categories_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
